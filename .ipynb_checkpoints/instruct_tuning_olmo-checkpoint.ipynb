{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f600fcc-c6bf-4bb1-b920-9ce58872c2cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## basics of instruction tuning with olmo 1b blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f1d1dd-1345-4379-8674-968804df7b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import hf_olmo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb41541-6e18-4d8d-8672-d1761f06f6ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ckpt = \"allenai/OLMo-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87d7348d-127d-4571-be3f-d8945f3cb900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ca7f1a-46f9-431e-b733-80ed88c536b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Some weights of OLMoForCausalLM were not initialized from the model checkpoint at allenai/OLMo-1B and are newly initialized: ['model.transformer.ff_out.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_ckpt,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15578264-9131-457c-9fe6-ee6c821bfe0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_new_tokens=100):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    return tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b0d0954-6ad2-462b-8dcd-9f262282f8af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Whats the capital of India?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c3ee24d-d75f-4f5b-abe8-9547b66d7b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whats the capital of India?\n",
      "Which country is known as the land of seven rivers?\n",
      "How many states are there in India?\n",
      "Is India a developed or developing country?\n",
      "Who is the richest man in India?\n",
      "The capital city of India is New Delhi. The official language of India is Hindi, but English is also widely spoken.\n",
      "India has been called “the world’s largest democracy” and “a nation with a rich culture.” It is home to more than 1 billion people, making it one\n"
     ]
    }
   ],
   "source": [
    "print(generate(prompt, max_new_tokens=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27af1c04-8bd1-4b49-bba2-920b9b758e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France?\n",
      "How many countries are in Europe?\n",
      "Is there a country called Russia?\n",
      "Which European country has the most land area?\n",
      "Where is the largest city in Europe?\n",
      "The United Kingdom, with an estimated population of about 65 million people, is the world’s second-largest country by total area.\n",
      "It is also the world’s sixth-most populous country and the most densely populated country..\n",
      "France is one of the oldest nations in Europe, having been founded as a\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"What is the capital of France?\", max_new_tokens=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac6ba33b-bf2c-4210-9bd7-fd8f7205da22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The captial of France is Paris.\n",
      "What is the capital of France?\n",
      "Paris, France is the capital city of France.\n",
      "Where is the capital of France located?\n",
      "France's capital is Paris.\n",
      "Is France a country or a state?\n",
      "France is a country and not a state.\n",
      "How many countries are in Europe?\n",
      "There are currently 27 European countries.\n",
      "Which country has the most land area?\n",
      "Italy has the most land area.\n",
      "What is the capital of Italy?\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"The captial of France is\", max_new_tokens=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2c269-a11e-4e36-b1e3-73cf019d4b1f",
   "metadata": {},
   "source": [
    "## download the hugggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31335c53-b1a2-4490-9ce7-6f4391995269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1a239b1-f6b5-4891-83c9-41a4f554b8b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache_dir = 'slimorca/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ea5c9bd-ba64-445c-b819-8e3619aa26d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slimorca = load_dataset('Open-Orca/SlimOrca',\n",
    "                           cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe98cfdd-6587-4116-a71d-36095a3a4671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8de736f9-93f4-408e-80d5-abca19a75a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"conversations\": [\n",
      "        {\n",
      "            \"from\": \"system\",\n",
      "            \"value\": \"You are an AI assistant. You will be given a task. You must generate a detailed and long answer.\",\n",
      "            \"weight\": null\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"human\",\n",
      "            \"value\": \"After World War II, the Guam Organic Act of 1950 established Guam as an unincorporated organized territory of the United States, provided for the structure of the island's civilian government, and granted the people U.S. citizenship. The Governor of Guam was federally appointed until 1968, when the Guam Elective Governor Act provided for the office's popular election.:242 Since Guam is not a U.S. state, U.S. citizens residing on Guam are not allowed to vote for president and their congressional representative is a non-voting member.\\nTry to answer this question if possible (otherwise reply \\\"unanswerable\\\"): In what year did World War II end?\",\n",
      "            \"weight\": 0.0\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"gpt\",\n",
      "            \"value\": \"World War II ended in 1945.\",\n",
      "            \"weight\": 1.0\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(slimorca[\"train\"][121], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e05073c-07ce-4411-a911-60b4182aab22",
   "metadata": {},
   "source": [
    "## formatting the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d79a649d-b702-4b00-bb89-0d75a0c7b06e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(tokenizer.chat_template), print(tokenizer.default_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "531a0ff8-a911-472e-948e-959902b33f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant and an expert at making coffee.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I make coffee with a Chemex coffee maker?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"To make coffee with a Chemex:\\n1. Boil water to about 200°F (93°C).\\n2. Place the Chemex filter in the top and rinse it with hot water to remove paper taste and warm the vessel. Discard the rinse water.\\n3. Add coffee grounds to the filter. Use a medium-coarse grind, about 1 gram of coffee per 16 grams of water.\\n4. Pour just enough hot water to saturate the grounds. Wait 30 seconds for the coffee to 'bloom'.\\n5. Slowly pour the remaining water over the grounds in a circular motion. Aim for a total brew time of 3.5 to 4.5 minutes.\\n6. Once brewing is complete, remove the filter and enjoy.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f584d13-eaab-4c1b-a2fc-dcbafe168553",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant and an expert at making coffee.<|im_end|>\n",
      "<|im_start|>user\n",
      "How do I make coffee with a Chemex coffee maker?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To make coffee with a Chemex:\n",
      "1. Boil water to about 200°F (93°C).\n",
      "2. Place the Chemex filter in the top and rinse it with hot water to remove paper taste and warm the vessel. Discard the rinse water.\n",
      "3. Add coffee grounds to the filter. Use a medium-coarse grind, about 1 gram of coffee per 16 grams of water.\n",
      "4. Pour just enough hot water to saturate the grounds. Wait 30 seconds for the coffee to 'bloom'.\n",
      "5. Slowly pour the remaining water over the grounds in a circular motion. Aim for a total brew time of 3.5 to 4.5 minutes.\n",
      "6. Once brewing is complete, remove the filter and enjoy.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)) ## understanding the chat template format\n",
    "## ChatML format #combines different types of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c51415b8-d2e1-43b9-88b0-1a2ece8b206f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e15b469-a01b-4692-92ac-d4e8da4e70d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "special_tokens = [\"<|im_start|>\", \"<|im_end|>\"] ## adding instruction tokens to the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "677c575f-c957-41d7-a357-7aa22d8337de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c56640f-3201-48b6-afa1-b5f468504ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_slimorca(ex):\n",
    "    role_mapping = {\"gpt\": \"assistant\", \"system\": \"system\", \"human\": \"user\"}\n",
    "    chat = [\n",
    "        {\"role\": role_mapping[message[\"from\"]], \"content\": message[\"value\"]}\n",
    "        for message in ex['conversations']\n",
    "    ]\n",
    "    formatted_chat = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )+ tokenizer.eos_token \n",
    "    tokenized_output = tokenizer(\n",
    "            formatted_chat,\n",
    "            add_special_tokens = False,\n",
    "             padding=\"max_length\",\n",
    "            max_length=1024,\n",
    "            truncation=True\n",
    "    )\n",
    "    return tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc67b08f-38f4-4576-a024-2ee438a9e1ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07031a8c95a4729a79e351c3aad5f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/517982 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "slimorca_tokenized = slimorca.map(format_slimorca, num_proc=16).remove_columns(\n",
    "    \"conversations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a12fdee2-9910-41bc-ae7b-f73f1c765533",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 517982\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slimorca_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9bafab1f-9629-4df7-8c3f-ba84b53ab78c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97271908-f716-4944-ada6-ccb7264cec5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator= DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3dc3dda-0b08-449d-8678-c9278f8751fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "slimorca_tokenized_split = slimorca_tokenized[\"train\"].train_test_split(\n",
    "    train_size=10000, test_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4bd127fa-00ea-405e-b0dc-a06857bf19d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e972ad5-13fa-40b1-af52-6c0e1772c26e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0022382e-6b1d-4ac8-8a19-abd63a1ab97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_tracking_path = os.path.join('mlflow_results', 'olmo_orcaslim_instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef12e4d5-f520-4b44-a752-6837e15593cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(mlflow_tracking_path):\n",
    "    os.mkdir(mlflow_tracking_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c710ce1e-5bb4-4ef8-8d2f-4835fd3b1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.path.join(mlflow_tracking_path, 'output')\n",
    "LOG_DIR = os.path.join(mlflow_tracking_path, \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4661eeb-4668-4ce9-b988-c3dc1d7590fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=4,\n",
    "    auto_find_batch_size=True,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=LOG_DIR,\n",
    "    logging_steps=5,  # Log every 5 steps\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=False,\n",
    "    save_steps=10000,\n",
    "    learning_rate=8.5e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0aedf06-c524-4514-8a7e-ab6111343c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=slimorca_tokenized_split[\"train\"],\n",
    "    eval_dataset=slimorca_tokenized_split[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d70f03c8-7faa-417e-81c5-fde2c912bf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment 'output'. Detailed error Yaml file 'mlflow_results/olmo_orcaslim_instruct/output/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/snoronha/conda/envs/venv/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 302, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"/home/snoronha/conda/envs/venv/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 395, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/snoronha/conda/envs/venv/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 1320, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/snoronha/conda/envs/venv/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 1313, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/snoronha/conda/envs/venv/lib/python3.9/site-packages/mlflow/utils/file_utils.py\", line 310, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'mlflow_results/olmo_orcaslim_instruct/output/meta.yaml' does not exist.\n",
      "2024/04/10 13:52:37 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 10:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='390' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [390/390 48:48, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.583700</td>\n",
       "      <td>1.619634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.510300</td>\n",
       "      <td>1.578189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.456600</td>\n",
       "      <td>1.565118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.436100</td>\n",
       "      <td>1.561697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.462900</td>\n",
       "      <td>1.561468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/10 14:42:00 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2024/04/10 14:42:00 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlflow.set_tracking_uri(mlflow_tracking_path)\n",
    "mlflow.set_experiment(\"slimorca_dataset_training\")\n",
    "with mlflow.start_run(log_system_metrics=True):\n",
    "    mlflow.log_params(training_args.to_dict())\n",
    "    trainer.evaluate() # eval before starting tuning\n",
    "    trainer.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37bfcf1d-b70f-4d8f-a00a-6bed10cddbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'model.transformer.ff_out.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('models/olmo_instruct_slimorca/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a0e0c0-86c3-42a8-87ad-8c82969a5f3f",
   "metadata": {},
   "source": [
    "## generating text using trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf278208-bc13-4a52-b481-ea7e4c2df347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Some weights of OLMoForCausalLM were not initialized from the model checkpoint at models/olmo_instruct_slimorca/ and are newly initialized: ['model.transformer.ff_out.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'models/olmo_instruct_slimorca/',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "00ab3e9c-67b9-4e10-a5f5-b03521d64cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_new_tokens=1000, chat=True):\n",
    "    if chat:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You have been fine-tuned to answer questions and follow instructions. Concisely answer the following instruction or question.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    else:\n",
    "        formatted_prompt = prompt\n",
    "    input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(\n",
    "        fine_tuned_model.device\n",
    "    )\n",
    "    gen_tokens = fine_tuned_model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    return tokenizer.batch_decode(gen_tokens, skip_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f4c480b4-b274-4a10-afd4-9e680670fb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You have been fine-tuned to answer questions and follow instructions. Concisely answer the following instruction or question.<|im_end|>\n",
      "<|im_start|>user\n",
      "can you tell me more about airbus<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Airbus is a French multinational corporation that designs, manufactures, and sells aerospace and defense products, including aircraft, satellites, and missiles. They are headquartered in Toulouse, France. Airbus has been involved in various projects related to space exploration, such as the International Space Station (ISS) and the European Space Agency's (ESA) ExoMars mission.\n",
      "\n",
      "To summarize this information: Airbus is an international company with a focus on space exploration and other advanced technologies. Their involvement in these projects demonstrates their commitment to innovation and excellence in the field of aerospace engineering.<|im_end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"can you tell me more about airbus\", chat=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "46aeb131-b02d-48e4-ba16-3c500efdda40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You have been fine-tuned to answer questions and follow instructions. Concisely answer the following instruction or question.<|im_end|>\n",
      "<|im_start|>user\n",
      "can you tell me more about airbus<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Airbus is a French multinational corporation that designs, manufactures, and sells aerospace and defense products, including aircraft, satellites, and missiles. They are headquartered in Toulouse, France. Airbus has been involved in various projects related to space exploration, such as the International Space Station (ISS) and the European Space Agency's (ESA) ExoMars mission.\n",
      "\n",
      "To summarize this information: Airbus is an international company with a focus on space exploration and other advanced technologies. Their involvement in these projects demonstrates their commitment to innovation and excellence in the field of aerospace engineering.<|im_end|>\n",
      "<|im_start|>user\n",
      "Which sector do they operate in?<|im_end|>\n",
      "\n",
      "<|endoftext|>The first thing I did when I got home was take my dog out for a walk. It was a beautiful day, so we went for a nice long walk around our neighborhood. We walked past some houses that were all lit up with Christmas lights, which made me think of how much fun it would be to decorate our house for Christmas!\n",
      "I also took a look at my Christmas decorations from last year. I had put them away after Thanksgiving because I didn’t want to deal with them anymore. But now that it’s December, I decided to bring them back out again. I love having pretty things around the house during the holidays, especially since I don’t get to see many people over the winter months.\n",
      "After taking my dog out for his walk, I sat down to watch TV. I usually like to watch something lighthearted while I relax, but I wanted to watch something more serious this time. So I settled on “The Good Place” on Netflix. This show is about a group of people who live in a place called The Good Place where everyone gets to go to heaven after they die. It’s very funny and entertaining, but it also touches on important topics like forgiveness and redemption.\n",
      "When I finished watching “The Good Place,” I felt really good. I knew that I had accomplished something today – I had done something productive even though it wasn’t necessarily what I set out to do. And I felt happy knowing that I had taken care of myself by doing something enjoyable and relaxing.\n",
      "So if you ever feel like your life isn’t going well, remember that there are always ways to make yourself feel better. You can find joy in simple things like watching TV shows, going for walks outside, or spending time with friends and family. These activities will help you feel better and give you something positive to focus on instead of focusing on problems in your life.\n",
      "In conclusion, I would say that the best way to feel better is to do something that makes you happy and relaxed. Whether it’s watching TV shows, going for walks outdoors, or spending time with friends and family, these activities will help you feel better and give you something positive to focus on instead of focusing on problems in your life. Remember that happiness doesn’t come from solving every problem in your life; rather, it comes from finding small moments of joy and satisfaction throughout each day. By doing this, you will eventually start feeling better overall.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "## does not working with multi step chat\n",
    "formatted_prompt = \"\"\"<|im_start|>system\n",
    "You have been fine-tuned to answer questions and follow instructions. Concisely answer the following instruction or question.<|im_end|>\n",
    "<|im_start|>user\n",
    "can you tell me more about airbus<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Airbus is a French multinational corporation that designs, manufactures, and sells aerospace and defense products, including aircraft, satellites, and missiles. They are headquartered in Toulouse, France. Airbus has been involved in various projects related to space exploration, such as the International Space Station (ISS) and the European Space Agency's (ESA) ExoMars mission.\n",
    "\n",
    "To summarize this information: Airbus is an international company with a focus on space exploration and other advanced technologies. Their involvement in these projects demonstrates their commitment to innovation and excellence in the field of aerospace engineering.<|im_end|>\n",
    "<|im_start|>user\n",
    "Which sector do they operate in?<|im_end|>\n",
    "\n",
    "<|endoftext|>\"\"\"\n",
    "input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(\n",
    "        fine_tuned_model.device\n",
    "    )\n",
    "gen_tokens = fine_tuned_model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "print(tokenizer.batch_decode(gen_tokens, skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819bb307-c3b0-4fda-a994-876d78d51fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
